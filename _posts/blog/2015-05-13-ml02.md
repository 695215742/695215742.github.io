---
layout: post
title: Machine Learning02:Gradient Descent Algorithm
description: 
category: blog
---

#####Gradient Descent Algorithm

* 	目的：求出局部最优的hypothesis:H(x)(使cost function J(θ1,θ2,...) 局部最优)

*	定义:
		* cost function：J(θ1,θ2,...)
		* J(θ1,θ2,...)的梯度(即导数):d(J)/d(θ), 如d(J)(θ1,θ2,...)/d(θ1)即J在(θ1,θ2,...)处关于θ1的导数

*	步骤

		* (1):选取一个起点:Point:(a1,b1,...)，J(a1,b1,...)
		* (2) θ1 := θ1 - α * d(J)/d(θ1)(a1,b1,...)，不断替换θ1,θ2,... ，直到最终收敛(convergence)。
		
对于复杂的函数，Gradient Descent只能找到一个局部最优的解，但是对于Hypothesis 是 linear function的情况，Gradient肯定能找到一个全局最优的解。

如图，对于y = θx+θ0这种形式的hypothesis，θ取值不同对应的cost function值也不同，如图一和图二所示，gradient descent就是要找到图中的最低点

![01](http://picturereq.herokuapp.com/images/coursera/ml_03.png)

![02](http://picturereq.herokuapp.com/images/coursera/ml_04.png)

matlab示例

``

theta_for = theta;
for iter = 1:num_iters
    for thetaindex = 1:size(X,2)
        theta(thetaindex) = theta(thetaindex) - alpha / m * sum((X * theta_for - y) .* X(:,thetaindex));
    end
    theta_for = theta;
    J_history(iter) = computeCostMulti(X, y, theta);
end

``

α控制每一步迈出的大小

* 太小增加了循环的次数

![01](http://picturereq.herokuapp.com/images/coursera/ml_07.png)

* 太大会越过optimize point，无法达到convergence

![02](http://picturereq.herokuapp.com/images/coursera/ml_06.png)

#####Batch :

Batch GD Algorithm是指使用了所有数据的GD Algorithm

